# =======================================================================
# Platform Name            platys-platform
# Platform Stack:          trivadis/platys-modern-data-platform
# Platform Stack Version:  develop
# =======================================================================
networks:
  default:
    name: platys-platform
# enforce some dependencies
# enforce some dependencies
services:
#  ================================== Kafka ========================================== #
  kafka-1:
    image: confluentinc/cp-kafka:8.1.1
    container_name: kafka-1
    hostname: kafka-1
    labels:
      com.platys.name: kafka
      com.platys.description: Kafka Broker 1
    ports:
      - 9092:9092
      - 19092:19092
      - 29092:29092
      - 39092:39092
      - 9992:9992
      - 1234:1234
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_BROKER_RACK: rack1
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-1:49092,2@kafka-2:49093,3@kafka-3:49094
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 1
      CLUSTER_ID: y4vRIwfDT0SkZ65tD7Ey2A
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,BROKER:PLAINTEXT,LOCAL:PLAINTEXT,DOCKERHOST:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: CONTROLLER://kafka-1:49092,BROKER://kafka-1:19092,LOCAL://kafka-1:39092,DOCKERHOST://kafka-1:29092,EXTERNAL://kafka-1:9092
      KAFKA_ADVERTISED_LISTENERS: BROKER://kafka-1:19092,LOCAL://localhost:39092,DOCKERHOST://${DOCKER_HOST_IP:-127.0.0.1}:29092,EXTERNAL://${PUBLIC_IP:-127.0.0.1}:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_MESSAGE_TIMESTAMP_TYPE: CreateTime
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_DELETE_TOPIC_ENABLE: 'True'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'False'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_JMX_PORT: 9992
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9992
      KAFKA_JMX_HOSTNAME: ${PUBLIC_IP:-127.0.0.1}
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_TOOLS_LOG4J_LOGLEVEL: INFO
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
    healthcheck:
      test: kafka-topics --bootstrap-server kafka-1:19092 --list
      interval: 30s
      timeout: 10s
      retries: 25
      start_period: 20s
  kafka-2:
    image: confluentinc/cp-kafka:8.1.1
    container_name: kafka-2
    hostname: kafka-2
    labels:
      com.platys.name: kafka
      com.platys.description: Kafka Broker 2
    ports:
      - 9093:9093
      - 19093:19093
      - 29093:29093
      - 39093:39093
      - 9993:9993
      - 1235:1234
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_BROKER_RACK: rack1
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-1:49092,2@kafka-2:49093,3@kafka-3:49094
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 2
      CLUSTER_ID: y4vRIwfDT0SkZ65tD7Ey2A
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,BROKER:PLAINTEXT,LOCAL:PLAINTEXT,DOCKERHOST:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: CONTROLLER://kafka-2:49093,BROKER://kafka-2:19093,LOCAL://kafka-2:39093,DOCKERHOST://kafka-2:29093,EXTERNAL://kafka-2:9093
      KAFKA_ADVERTISED_LISTENERS: BROKER://kafka-2:19093,LOCAL://localhost:39093,DOCKERHOST://${DOCKER_HOST_IP:-127.0.0.1}:29093,EXTERNAL://${PUBLIC_IP:-127.0.0.1}:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_MESSAGE_TIMESTAMP_TYPE: CreateTime
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_DELETE_TOPIC_ENABLE: 'True'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'False'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_JMX_PORT: 9993
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9993
      KAFKA_JMX_HOSTNAME: ${PUBLIC_IP:-127.0.0.1}
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_TOOLS_LOG4J_LOGLEVEL: INFO
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
    healthcheck:
      test: kafka-topics --bootstrap-server kafka-1:19092 --list
      interval: 30s
      timeout: 10s
      retries: 25
      start_period: 20s
  kafka-3:
    image: confluentinc/cp-kafka:8.1.1
    container_name: kafka-3
    hostname: kafka-3
    labels:
      com.platys.name: kafka
      com.platys.description: Kafka Broker 3
    ports:
      - 9094:9094
      - 19094:19094
      - 29094:29094
      - 39094:39094
      - 9994:9994
      - 1236:1234
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_BROKER_RACK: rack1
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-1:49092,2@kafka-2:49093,3@kafka-3:49094
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 3
      CLUSTER_ID: y4vRIwfDT0SkZ65tD7Ey2A
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,BROKER:PLAINTEXT,LOCAL:PLAINTEXT,DOCKERHOST:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: CONTROLLER://kafka-3:49094,BROKER://kafka-3:19094,LOCAL://kafka-3:39094,DOCKERHOST://kafka-3:29094,EXTERNAL://kafka-3:9094
      KAFKA_ADVERTISED_LISTENERS: BROKER://kafka-3:19094,LOCAL://localhost:39094,DOCKERHOST://${DOCKER_HOST_IP:-127.0.0.1}:29094,EXTERNAL://${PUBLIC_IP:-127.0.0.1}:9094
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_MESSAGE_TIMESTAMP_TYPE: CreateTime
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_DELETE_TOPIC_ENABLE: 'True'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'False'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_JMX_PORT: 9994
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9994
      KAFKA_JMX_HOSTNAME: ${PUBLIC_IP:-127.0.0.1}
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_TOOLS_LOG4J_LOGLEVEL: INFO
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
    healthcheck:
      test: kafka-topics --bootstrap-server kafka-1:19092 --list
      interval: 30s
      timeout: 10s
      retries: 25
      start_period: 20s
  #  ================================== Kafka Connect ========================================== #
  kafka-connect-1:
    image: confluentinc/cp-kafka-connect:8.1.1
    container_name: kafka-connect-1
    hostname: kafka-connect-1
    labels:
      com.platys.name: kafka-connect
      com.platys.description: Kafka Connect Node 1
      com.platys.restapi.title: Kafka Connect REST API
      com.platys.restapi.url: http://dataplatform:8083
    ports:
      - 8083:8083
    depends_on:
      kafka-1:
        condition: service_healthy
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka-1:19092,kafka-2:19093,kafka-3:19094
      CONNECT_LISTENERS: http://0.0.0.0:8083
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect-1
      CONNECT_REST_ADVERTISED_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect-cluster
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_ACCESS_CONTROL_ALLOW_ORIGIN: '*'
      CONNECT_ACCESS_CONTROL_ALLOW_METHODS: GET,POST,PUT,DELETE
      CONNECT_ACCESS_CONTROL_ALLOW_HEADERS: origin,content-type,accept,authorization
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: '[%d] %p %X{connector.context}%m (%c:%L)%n'
      CONNECT_PLUGIN_PATH: /usr/share/java,/etc/kafka-connect/addl-plugins,/etc/kafka-connect/cflthub-plugins
      CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY: All
      CONNECT_SECURITY_PROTOCOL: PLAINTEXT
      #CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-4.0.0.jar
      AWS_ACCESS_KEY_ID: ${PLATYS_AWS_ACCESS_KEY:-admin}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123!}
      # External secrets config
      # See https://docs.confluent.io/current/connect/security.html#externalizing-secrets
      CONNECT_CONFIG_PROVIDERS: file
      CONNECT_CONFIG_PROVIDERS_FILE_CLASS: org.apache.kafka.common.config.provider.FileConfigProvider
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/kafka-connect/connectors:/etc/kafka-connect/addl-plugins
      - ./plugins/kafka-connect/jars:/etc/kafka-connect/jars
      - ./plugins/opentelemetry/agents:/otel
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
        echo "Installing Connectors"
        mkdir -p /etc/kafka-connect/cflthub-plugins
        for i in $$(echo "confluentinc/kafka-connect-jdbc:10.8.4,debezium/debezium-connector-postgresql:3.1.2,debezium/debezium-connector-mysql:3.1.2,confluentinc/kafka-connect-s3:11.0.0,confluentinc/kafka-connect-elasticsearch:15.1.1" | sed "s/,/ /g")
        do
          confluent-hub install --no-prompt --component-dir /etc/kafka-connect/cflthub-plugins --verbose "$$i"
        done
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run &
        #
        sleep infinity
    restart: unless-stopped
    healthcheck:
      interval: 30s
      retries: 25
      test: curl --user connectAdmin:connectAdmin --fail --silent --insecure https://kafka-connect-1:8083/ --output /dev/null || exit 1
  #  ================================== ksqlDB ========================================== #
  ksqldb-server-1:
    image: confluentinc/cp-ksqldb-server:8.1.1
    hostname: ksqldb-server-1
    container_name: ksqldb-server-1
    labels:
      com.platys.name: ksqldb
      com.platys.description: ksqlDB Streaming Database - Node 1
      com.platys.restapi.title: ksqlDB Server REST API
      com.platys.restapi.url: http://dataplatform:8088
    depends_on:
      kafka-1:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
    ports:
      - 8088:8088
      - 1095:1095
    environment:
      UB_CLASSPATH: /usr/share/java/confluent-security/ksql/*:/usr/share/java/ksqldb-server/*:/usr/share/java/cp-base-new/*
      KSQL_LOG4J_ROOT_LOGLEVEL: INFO
      KSQL_LOG4J_OPTS: -Dlog4j.configuration=file:/tmp/helper/log4j.properties
      KSQL_LOG4J_LOGGERS: org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR
      KSQL_LOG4J_PROCESSING_LOG_TOPIC: ksql_processing_log
      KSQL_LOG4J_PROCESSING_LOG_BROKERLIST: kafka-1:19092,kafka-2:19093,kafka-3:19094
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_NAME: ksql_processing_log
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 1
      KSQL_KSQL_LOGGING_PROCESSING_ROWS_INCLUDE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
      # For Demo purposes: improve resource utilization and avoid timeouts
      KSQL_KSQL_STREAMS_NUM_STREAM_THREADS: 1
      KSQL_PRODUCER_ENABLE_IDEMPOTENCE: 'true'
      KSQL_KSQL_PERSISTENCE_DEFAULT_FORMAT_KEY: KAFKA
      KSQL_APPLICATION_ID: ksqldb-cluster
      KSQL_KSQL_SERVICE_ID: ksqldb-cluster
      KSQL_HOST_NAME: ksqldb-server-1
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: kafka-1:19092,kafka-2:19093,kafka-3:19094
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      KSQL_RESPONSE_HTTP_HEADERS_CONFIG: ''
      KSQL_SECURITY_PROTOCOL: PLAINTEXT
      KSQL_KSQL_CONNECT_URL: http://kafka-connect-1:8083
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      KSQL_KSQL_INTERNAL_TOPIC_REPLICAS: 1
      KSQL_KSQL_SINK_REPLICAS: 1
      KSQL_KSQL_STREAMS_REPLICATION_FACTOR: 1
      KSQL_KSQL_QUERY_PULL_METRICS_ENABLED: 'true'
      KSQL_KSQL_HIDDEN_TOPICS: ^_.*,default_ksql_processing_log
      KSQL_KSQL_SUPPRESS_ENABLED: 'False'
      KSQL_KSQL_SUPPRESS_BUFFER_SIZE_BYTES: '-1'
      KSQL_KSQL_QUERY_PULL_TABLE_SCAN_ENABLED: 'False'
      KSQL_CONFIG_DIR: /etc/ksqldb
      KSQL_KSQL_EXTENSION_DIR: /etc/ksqldb/ext/
      KSQL_JMX_OPTS: -Djava.rmi.server.hostname=localhost -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1095 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=1095
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/ksqldb:/etc/ksqldb/ext
      - ./plugins/kafka-connect/connectors:/etc/kafka-connect/addl-plugins
      - ./plugins/kafka-connect/jars:/etc/kafka-connect/jars
      - ./plugins/opentelemetry/agents:/otel
    restart: unless-stopped
    healthcheck:
      start_period: 10s
      interval: 10s
      retries: 25
      test: curl --user ksqlDBUser:ksqlDBUser -fail --silent http://ksqldb-server-1:8088/info | grep RUNNING 1>/dev/null || exit 1
  # Access the cli by running:
  # > docker exec -it ksqldb-cli ksql http://ksqldb-server-1:8088
  ksqldb-cli:
    image: confluentinc/cp-ksqldb-server:8.1.1
    container_name: ksqldb-cli
    hostname: ksqldb-cli
    labels:
      com.platys.name: ksqldb-cli
      com.platys.description: ksqlDB Command Line Utility
    depends_on:
      - ksqldb-server-1
    volumes:
      - ./data-transfer:/data-transfer
    entrypoint: /bin/sh
    tty: true
    restart: unless-stopped
  #  ================================== Confluent Schema Registry ========================================== #
  schema-registry-1:
    image: confluentinc/cp-schema-registry:8.1.1
    hostname: schema-registry-1
    container_name: schema-registry-1
    labels:
      com.platys.name: schema-registry
      com.platys.description: Confluent Schema Registry
      com.platys.restapi.title: Schema Registry REST API
      com.platys.restapi.url: http://dataplatform:8081
    ports:
      - 8081:8081
    depends_on:
      kafka-1:
        condition: service_healthy
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry-1
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-1:19092,kafka-2:19093,kafka-3:19094
      SCHEMA_REGISTRY_GROUP_ID: schema-registry
      SCHEMA_REGISTRY_LEADER_ELIGIBILITY: 'True'
      SCHEMA_REGISTRY_MODE_MUTABILITY: 'True'
      SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: backward
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 1
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: '*'
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: GET,POST,PUT,OPTIONS
      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: info
      SCHEMA_REGISTRY_DEBUG: 'False'
      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: PLAINTEXT
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/opentelemetry/agents:/otel
    restart: unless-stopped
    healthcheck:
      start_period: 10s
      interval: 15s
      retries: 25
      test: curl --fail --silent --insecure http://schema-registry-1:8081/subjects --output /dev/null || exit 1
  #  ================================== Jikkou ========================================== #
  jikkou:
    image: streamthoughts/jikkou:latest
    container_name: jikkou
    hostname: jikkou
    labels:
      com.platys.name: jikkou
      com.platys.description: Resource as Code framework for Apache Kafka
    environment:
      JIKKOU_DEFAULT_KAFKA_BOOTSTRAP_SERVERS: kafka-1:19092,kafka-2:19093,kafka-3:19094
      JIKKOU_DEFAULT_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      JIKKOU_KAFKA_BROKERS_WAIT_FOR_MIN_AVAILABLE: 3
      JIKKOU_KAFKA_BROKERS_WAIT_FOR_ENABLED: 'true'
      JIKKOU_KAFKA_BROKERS_WAIT_FOR_RETRY_BACKOFF_MS: 10000
      JIKKOU_KAFKA_BROKERS_WAIT_FOR_TIMEOUT_MS: 120000
      VALIDATION_DEFAULT_TOPIC_NAME_REGEX: '[a-zA-Z0-9\._\-]+'
      VALIDATION_DEFAULT_TOPIC_MIN_NUM_PARTITIONS: 1
      VALIDATION_DEFAULT_TOPIC_MIN_REPLICATION_FACTOR: 1
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/jikkou/:/jikkou
      - ./conf/jikkou/application.conf:/appuser/.jikkou/application.conf
      - ./conf/jikkou/config:/etc/jikkou/config
    command:
      - apply
      - --files
      - /jikkou/
      - --file-name
      - '**/*.{yaml,yml}'
    restart: on-failure
  #  ================================== Kafka Connect UI ========================================== #
  kafka-connect-ui:
    image: landoop/kafka-connect-ui:latest
    container_name: kafka-connect-ui
    hostname: kafka-connect-ui
    labels:
      com.platys.name: kafka-connect-ui
      com.platys.description: Kafka Connect GUI
      com.platys.webui.title: Kafka Connect UI
      com.platys.webui.url: http://dataplatform:28103
    ports:
      - 28103:8000
    environment:
      CONNECT_URL: http://kafka-connect-1:8083
      PROXY: 'true'
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/kafka-connect-ui/resolv.conf:/etc/resolv.conf:ro
    restart: unless-stopped
  #  ================================== Apache Kafka HQ (AKHQ) ========================================== #
  akhq:
    image: tchiotludo/akhq:latest
    container_name: akhq
    hostname: akhq
    labels:
      com.platys.name: akhq
      com.platys.description: Kafka GUI
      com.platys.webui.title: AKHQ UI
      com.platys.webui.url: http://dataplatform:28107
      com.platys.restapi.title: AKHQ API
      com.platys.restapi.url: http://dataplatform:28107/api
      com.platys.password.envvars: PLATYS_AKHQ_ADMIN_PASSWORD,PLATYS_AKHQ_READER_PASSWORD
    ports:
      - 28107:8080
      - 28320:28081
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          pagination.page-size: 25
          topic-data:
            size: 50
            poll-timeout: 1000
            kafka-max-message-length: 1000000
          ui-options:
            topic:
              default-view: HIDE_INTERNAL
              skip-consumer-groups: false
              skip-last-record: true
              show-all-consumer-groups: true
            topic-data:
              sort: OLDEST
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: 'kafka-1:19092,kafka-2:19093,kafka-3:19094'
              schema-registry:
                url: "http://schema-registry-1:8081"
                type: "confluent"
              connect:
                - name: "connect-1"
                  url: "http://kafka-connect-1:8083"
              ksqldb:
                - name: "ksqldb"
                  url: "http://ksqldb-server-1:8088"
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== ShadowTraffic ========================================== #
  shadowtraffic:
    image: shadowtraffic/shadowtraffic:latest
    container_name: shadowtraffic
    hostname: shadowtraffic
    profiles: [test]
    labels:
      com.platys.name: shadowtraffic
      com.platys.description: Simulate production traffic to your backend.
    depends_on:
      schema-registry-1:
        condition: service_started
    extra_hosts:
      - host.docker.internal:host-gateway
    environment:
      - LICENSE_ID=dbbcf90d-fc9f-40b2-bcc3-48e615dea580
      - LICENSE_EMAIL=michael+examples@shadowtraffic.io
      - LICENSE_ORGANIZATION=ShadowTraffic
      - LICENSE_EDITION=ShadowTraffic Free Trial
      - LICENSE_EXPIRATION=2026-02-26
      - LICENSE_SIGNATURE=hWB97kIcve1gyMtks/QoiQu0B/EILqfPj+qBh2JuSbVO3vKOLJuPljVGstx9Gb8ay7CGdGwoPWYIOkgsWw1fh0xYROtGRWYeQu40n4EZ+wC8JzXXrOxFyRZGB1yWIiJwkX/5/AGvN3BOtkbFukzuJKyIh4fJp82HgbR/B7JcwM+Ht7RSL9YqWbT3YTr8IRhzlDSyllw5d94v9s0G2ALVk+/nwsFpubQKUHSmChUUtkhrnX71Nt/Yvynu5JYw3ntK8I6GutQpIoi6y7dvvziFAxtovcLppY3hBsAgW/tw+PAm1JAgte+n4Rf6vVOYDArFWhqT1YfbOsJCbidHjEHMAg==
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:19092,kafka-2:19093,kafka-3:19094
      - KAFKA_SCHEMA_REGISTRY_URL=http://schema-registry-1:8081
      - POSTGRESQL_HOST=postgresql
      - POSTGRESQL_PORT=5432
      - POSTGRESQL_DATABASE=postgres
      - POSTGRESQL_USER=postgres
      - POSTGRESQL_PASSWORD=abc123!
      - S3_ENDPOINT=http://minio-1:9000
      - S3_PATH_STYLE_ACCESS='true'
      - AWS_REGION=us-east-1
      - AWS_ACCESS_KEY_ID=${PLATYS_AWS_ACCESS_KEY:-admin}
      - AWS_SECRET_ACCESS_KEY=${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123!}
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/shadowtraffic:/data
    command: |
      --config /data/card-fraud.json
      --metrics-port 9400
      --reload completion
      --watch
      --seed 757928231
    restart: unless-stopped
  #  ================================== Apache Spark ========================================== #
  spark-master:
    image: bitnamilegacy/spark:3.5.3
    container_name: spark-master
    hostname: spark-master
    labels:
      com.platys.name: spark
      com.platys.description: Spark Master Node
      com.platys.webui.title: Spark UI
      com.platys.webui.url: http://dataplatform:28304
    ports:
      - 28304:28304
      - 6066:6066
      - 7077:7077
      - 4040-4044:4040-4044
    environment:
      # bitnami env vars
      SPARK_MODE: master
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
      # spark standard env vars
      SPARK_MASTER_WEBUI_PORT: 28304
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      # env vars for config files
#     INIT_DAEMON_STEP: setup_spark
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:-admin}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123!}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      AWS_REGION: us-east-1
      AWS_ACCESS_KEY_ID: ${PLATYS_AWS_ACCESS_KEY:-admin}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123!}
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_jars: builtin
      # SPARK_DEFAULTS_CONF_spark_sql_hive_metastore_version: 3.1.2    
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      SPARK_DEFAULTS_CONF_spark_sql_catalog_spark__catalog: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hive: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hive_uri: thrift://hive-metastore:9083
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hive_warehouse: s3a://admin-bucket/iceberg/warehouse
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hiverest_type: rest
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hiverest: org.apache.iceberg.spark.SparkCatalog
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hiverest_uri: http://hive-metastore:9084/iceberg
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hiverest_token: not_used
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hiverest_warehouse: s3a://admin-bucket/iceberg/warehouse
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hiverest_io___impl: org.apache.iceberg.aws.s3.S3FileIO
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hiverest_s3_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hiverest_s3_path___style___access: true
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hiverest_s3_access___key___id: ${PLATYS_AWS_ACCESS_KEY:-admin}
      SPARK_DEFAULTS_CONF_spark_sql_catalog_hiverest_s3_secret___access___key: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123!}
      SPARK_DEFAULTS_CONF_spark_sql_legacy_allowNonEmptyLocationInCTAS: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages: ''
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      # specifies the JARs to be downloaded to the jars folder by maven-download-sh script
      SPARK_INSTALL_JARS_PACKAGES: ',io.delta:delta-spark_2.12:3.3.2,io.delta:delta-storage:3.3.2,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.1,org.apache.iceberg:iceberg-aws-bundle:1.10.1,org.apache.hudi:hudi-spark3.4-bundle_2.12:0.15.0'
      SPARK_DEFAULTS_CONF_spark_jars: /opt/bitnami/spark/jars/iceberg-spark-runtime-3.5_2.12-1.10.1.jar
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
    volumes:
      - ./data-transfer:/data-transfer
      # the 3 conf files are mapped to conf.default folder, they are copied with env variable interpolation into conf upon start of container
      - ./conf/spark/hive-site.xml:/opt/bitnami/spark/conf.default/hive-site.xml
      - ./conf/spark/spark-defaults.conf:/opt/bitnami/spark/conf.default/spark-defaults.conf
      - ./init/spark:/docker-entrypoint-initdb.d
      - ./scripts/docker/maven-download.sh:/maven-download.sh
      - ./scripts/spark/pyspark:/opt/bitnami/spark/bin/pyspark
      - ./container-volume/spark/logs/:/var/log/spark/logs
#      - ./scripts/docker/maven-download.sh:/usr/src/app/maven-download.sh
      - spark-3-5-3-vol:/opt/bitnami/spark
      - spark-3-5-3-java-vol:/opt/bitnami/java
    restart: unless-stopped
    healthcheck:
      test: [CMD-SHELL, "python3 -c 'import socket; socket.create_connection((\"localhost\", 28304), timeout=5)'"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 20s
  #  ================================== Apache Hive Metastore ========================================== #
  hive-metastore:
    image: trivadis/apache-hive:4.2.0-postgresql-standalone-metastore-s3
    container_name: hive-metastore
    hostname: hive-metastore
    labels:
      com.platys.name: hive-metastore
      com.platys.description: Hive Metastore
    depends_on:
      hive-metastore-db:
        condition: service_healthy
    ports:
      - 9083:9083
      - 9084:9084
    environment:
      HIVE_VER: 4.2.0
      # configure iceberg catalog service (if HMS >= 4.1.0)
      HIVE_SITE_CONF_metastore_catalog_servlet_port: 9084
      HIVE_SITE_CONF_metastore_catalog_servlet_auth: none
      CORE_CONF_fs_defaultFS: file:///tmp
      HIVE_SITE_CONF_fs_defaultFS: file:///tmp
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:-admin}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123!}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      HIVE_SITE_CONF_hive_metastore_uris: thrift://hive-metastore:9083
      HIVE_SITE_CONF_hive_metastore_warehouse_dir: s3a://admin-bucket/warehouse
      HIVE_SITE_CONF_hive_metastore_warehouse_external_dir: s3a://admin-bucket/warehouse
      HIVE_SITE_CONF_javax_jdo_option_ConnectionURL: jdbc:postgresql://hive-metastore-db/metastore_db
      HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName: org.postgresql.Driver
      HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName: hive
      HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword: ${PLATYS_HMS_POSTGRESQL_PASSWORD:-abc123!}
      HIVE_SITE_CONF_datanucleus_autoCreateSchema: false
      HIVE_SITE_CONF_hive_metastore_event_db_notification_api_auth: false
      # necessary for Trino to be able to read from Avro
      HIVE_SITE_CONF_metastore_storage_schema_reader_impl: org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader
      SERVICE_PRECONDITION: hive-metastore-db:5432
      DB_DRIVER: postgres
      SERVICE_NAME: standalone-metastore
      SERVICE_OPTS: -Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://hive-metastore-db/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=${PLATYS_HMS_POSTGRESQL_PASSWORD:-abc123!}
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
    healthcheck:
      test: [CMD, nc, -z, hive-metastore, '9083']
      interval: 30s
      timeout: 10s
      retries: 10
  hive-metastore-db:
    image: postgres
    container_name: hive-metastore-db
    hostname: hive-metastore-db
    labels:
      com.platys.name: hive-metastore
      com.platys.description: Hive Metastore DB
      com.platys.password.envvars: PLATYS_HMS_POSTGRESQL_PASSWORD
    ports:
      - 5442:5432
    environment:
      POSTGRES_DB: metastore_db
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: ${PLATYS_HMS_POSTGRESQL_PASSWORD:-abc123!}
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
    healthcheck:
      test: [CMD, psql, -U, hive, metastore_db]
  #  ================================== Jupyter ========================================== #
  jupyter:
    image: quay.io/jupyter/pyspark-notebook:spark-4.1.1
    container_name: jupyter
    hostname: jupyter
    labels:
      com.platys.name: jupyter
      com.platys.description: Web-based interactive development environment for notebooks, code, and data
      com.platys.webui.title: Jupyter UI
      com.platys.webui.url: http://dataplatform:28888
      com.platys.password.envvars: PLATYS_JUPYTER_TOKEN,PLATYS_AWS_SECRET_ACCESS_KEY
    ports:
      - 28888:8888
      - 28376-28380:4040-4044
    user: root
    extra_hosts:
      - host.docker.internal:host-gateway
    environment:
      JUPYTER_ENABLE_LAB: "'yes'"
      GRANT_SUDO: "'yes'"
      JUPYTER_TOKEN: ${PLATYS_JUPYTER_TOKEN:-abc123!}
      DOCKER_STACKS_JUPYTER_CMD: lab
      MAVEN_DOWNLOAD_JARS: software.amazon.awssdk:bundle:2.29.52,org.apache.hadoop:hadoop-aws:3.4.2,com.google.guava:guava:33.4.8-jre
      # remove some JARS if they are conflicting with the ones installed above
      REMOVE_JARS: guava-14.0.1.jar
      # for awscli & s3cmd
      AWS_ACCESS_KEY_ID: ${PLATYS_AWS_ACCESS_KEY:-admin}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123!}
      AWS_ENDPOINT: http://minio-1:9000
      AWS_REGION: us-east-1
      AWS_DEFAULT_REGION: us-east-1
      #PYSPARK_PYTHON: /opt/bitnami/python/bin/python3.12
      #PYSPARK_DRIVER_PYTHON: /usr/bin/python3.12   
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/jupyter/on-startup-jupyter/:/usr/local/bin/start-notebook.d/
      - ./init/jupyter/on-startup-jupyter-finished/:/usr/local/bin/before-notebook.d/
      - ./init/jupyter/on-startup-notebook-kernel:/home/jovyan/.ipython/profile_default/startup/
      - ./scripts/docker/maven-download.sh:/maven-download.sh
#      - spark-3-5-3-vol:/opt/bitnami/spark:RO
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
        #conda create -y --name py312 python=3.12.8
        #source /opt/conda/etc/profile.d/conda.sh
        #conda activate py312
        #pip install ipykernel    
        #python -m ipykernel install --user --name py312 --display-name "Python 3.12.8 (ipykernel)"
        pip install pyspark==4.1.1 jupysql protobuf==6.33.0 boto3
        start-notebook.sh
    restart: unless-stopped
  #  ================================== Elasticsearch ========================================== #
  elasticsearch-1:
    image: elasticsearch:8.19.10
    hostname: elasticsearch-1
    container_name: elasticsearch-1
    labels:
      com.platys.name: elasticsearch
      com.platys.description: Search-engine NoSQL store
      com.platys.restapi.title: Elasticsearch REST API
      com.platys.restapi.url: http://dataplatform:9200
      com.platys.manual.step.msgs: sudo sysctl -w vm.max_map_count=262144
    ports:
      - 9200:9200
      - 9300:9300
    environment:
      discovery.type: single-node
      xpack.security.enabled: false
      xpack.monitoring.collection.enabled: 'false'
      xpack.security.http.ssl.enabled: false
      xpack.security.transport.ssl.enabled: false
      http.cors.enabled: 'true'
      http.cors.allow-origin: http://${DOCKER_HOST_IP}:28275,http://${PUBLIC_IP}:28275,http://dejavu:1358,http://dataplatform:28125,http://dataplatform:28125,http://${PUBLIC_IP}:28125,http://${DOCKER_HOST_IP}:28125,http://127.0.0.1:1358
      http.cors.allow-headers: X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorization
      http.cors.allow-credentials: 'true'
      cluster.routing.allocation.disk.threshold_enabled: 'true'
      cluster.routing.allocation.disk.watermark.low: 2gb
      cluster.routing.allocation.disk.watermark.high: 1gb
      cluster.routing.allocation.disk.watermark.flood_stage: 512mb
      ES_JAVA_OPTS: -Xms512m -Xmx512m
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
    healthcheck:
      test:
        - CMD-SHELL
        - curl -s http://localhost:9200/_cat/health?h=status | grep -q green
      retries: 300
      interval: 10s
  #  ================================== Prometheus ========================================== #
  prometheus-1:
    image: prom/prometheus:v3.9.1
    container_name: prometheus-1
    hostname: prometheus-1
    labels:
      com.platys.name: prometheus
      com.platys.description: Monitoring system and time series database
      com.platys.webui.title: Prometheus UI
      com.platys.webui.url: http://dataplatform:9090/graph
      com.platys.restapi.title: Prometheus Rest API
      com.platys.restapi.url: http://dataplatform:9090/api/v1
    ports:
      - 9090:9090
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/prometheus/prometheus-config/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --web.console.libraries=/usr/share/prometheus/console_libraries
      - --web.console.templates=/usr/share/prometheus/consoles
      - --web.listen-address=0.0.0.0:9090
      - --storage.tsdb.retention.time=30d
      - --log.level=info
    restart: unless-stopped
  #  ================================== PostgreSQL ========================================== #
  postgresql:
    image: postgres:18
    container_name: postgresql
    hostname: postgresql
    labels:
      com.platys.name: postgresql
      com.platys.description: Open-Source object-relational database system
      com.platys.password.envvars: PLATYS_POSTGRESQL_PASSWORD,PLATYS_POSTGRESQL_MULTIPLE_PASSWORD
    ports:
      - 5432:5432
    environment:
      - POSTGRES_PASSWORD=${PLATYS_POSTGRESQL_PASSWORD:-abc123!}
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres
#      - POSTGRES_INITDB_ARGS=--encoding=UTF8 --data-checksums      
      - POSTGRES_MULTIPLE_DATABASES=customer_db
      - POSTGRES_MULTIPLE_USERS=customer
      - POSTGRES_MULTIPLE_PASSWORDS=${PLATYS_POSTGRESQL_MULTIPLE_PASSWORD:-abc123!}
      - POSTGRES_MULTIPLE_ADDL_ROLES=
      - PGDATA=/var/lib/postgresql/data/pgdata
      - DB_SCHEMA=demo
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/postgresql:/docker-entrypoint-initdb.d/
    command:
      - postgres
      - -c
      - wal_level=logical
      - -c
      - max_replication_slots=10
      - -c
      - max_wal_senders=10
    restart: unless-stopped
    healthcheck:
      test: [CMD-SHELL, pg_isready -U postgres]
      interval: 10s
      timeout: 5s
      retries: 5
  #  ================================== Presto ========================================== #
  presto-1:
    image: prestodb/presto:0.296
    hostname: presto-1
    container_name: presto-1
    labels:
      com.platys.name: presto
      com.platys.description: SQL Virtualization Engine
      com.platys.webui.title: Presto UI
      com.platys.webui.url: http://dataplatform:28081
    ports:
      - 28081:8080
    environment:
      COORDINATOR_HOST: presto-1
      HMS_ICEBERG_REST_CATALOG_URI: http://hive-metastore:9084/iceberg
      HMS_ICEBERG_REST_CATALOG_WAREHOUSE: s3a://admin-bucket/iceberg/warehouse
      S3_ENDPOINT: http://minio-1:9000
      S3_REGION: us-east-1
      S3_AWS_ACCESS_KEY: ${PLATYS_AWS_ACCESS_KEY:-admin}
      S3_AWS_SECRET_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123!}
      S3_PATH_STYLE_ACCESS: 'true'
      HIVE_METASTORE_URI: thrift://hive-metastore:9083
      HIVE_STORAGE_FORMAT: ORC
      HIVE_COMPRESSION_CODEC: GZIP
      NESSIE_CATALOG_WAREHOUSE_DIR: s3a://admin-bucket/nessie/warehouse
      KAFKA_NODES: kafka-1:19092,kafka-2:19093,kafka-3:19094
      KAFKA_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      KAFKA_CONFIG_RESOURCES: /opt/presto-server/etc/kafka-configuration.properties
      KAFKA_SECURITY_PROTOCOL: PLAINTEXT
      KAFKA_TABLE_NAMES:
      KAFKA_DEFAULT_SCHEMA: default
      KAFKA_TABLE_DESCRIPTOR_DIR: /opt/presto-server/etc/kafka/
      POSTGRESQL_DATABASE: postgres
      POSTGRESQL_USER: postgres
      POSTGRESQL_PASSWORD: abc123!
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/presto/single/config.properties:/opt/presto-server/etc/config.properties
      - ./conf/presto/single/node.properties:/opt/presto-server/etc/node.properties
      - ./conf/presto/catalog/minio.properties:/opt/presto-server/etc/catalog/minio.properties
      - ./conf/presto/catalog/iceberg-hive.properties:/opt/presto-server/etc/catalog/iceberg_hive.properties
      - ./conf/presto/catalog/iceberg-hive-rest.properties:/opt/presto-server/etc/catalog/iceberg_hive_rest.properties
      - ./conf/presto/catalog/postgresql.properties:/opt/presto-server/etc/catalog/postgresql.properties
      - ./conf/presto/catalog/elasticsearch.properties:/opt/presto-server/etc/catalog/elasticsearch.properties
      - ./conf/presto/catalog/prometheus.properties:/opt/presto-server/etc/catalog/prometheus.properties
    restart: unless-stopped
  #  ================================== Minio ========================================== #
  minio-1:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z
    container_name: minio-1
    hostname: minio-1
    labels:
      com.platys.name: minio
      com.platys.description: Software-defined Object Storage
      com.platys.webui.title: MinIO UI
      com.platys.webui.url: http://dataplatform:9010
      com.platys.password.envvars: PLATYS_AWS_SECRET_ACCESS_KEY
    ports:
      # S3 API Port
      - 9000:9000
      # UI Port
      - 9010:9010
    environment:
      MINIO_ROOT_USER: ${PLATYS_AWS_ACCESS_KEY:-admin}
      MINIO_ROOT_PASSWORD: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123!}
      # remove region due to problems with RisingWave
      #MINIO_REGION_NAME: us-east-1
      #MINIO_REGION: us-east-1
      MINIO_DOMAIN: minio
      MINIO_SERVER_URL: http://${PUBLIC_IP}:9000
      MINIO_COMPRESSION_ENABLE: off
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_PROMETHEUS_URL: http://prometheus-1:9090
    volumes:
      - ./data-transfer:/data-transfer
    command: server /data --console-address ":9010"
    restart: unless-stopped
    healthcheck:
      test: [CMD, curl, -f, http://minio-1:9000/minio/health/live]
      interval: 15s
      timeout: 20s
      retries: 3
  #  ================================== Minio MC ========================================== #
  minio-mc:
    image: minio/mc:latest
    container_name: minio-mc
    hostname: minio-mc
    labels:
      com.platys.name: minio
      com.platys.description: MinIO Console
    environment:
      # these two env variables are also needed for the s3-credentials.properties file gen to work! 
      AWS_ACCESS_KEY: ${PLATYS_AWS_ACCESS_KEY:-admin}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123!}
      MC_HOST_minio-1: http://${PLATYS_AWS_ACCESS_KEY:-admin}:${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123!}@minio-1:9000
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
      - ./security/aws/credentials:/tmp/credentials.templ
      - aws-credentials-vol:/tmp/.aws:RO
    entrypoint:
      - /bin/sh
      - -c
      - |
        /usr/src/app/wait-for-it.sh -t 180 minio-1:9000
        mkdir -p /tmp/.aws
        eval "echo \"$$(cat /tmp/credentials.templ)\"" >> /tmp/.aws/credentials
        mc mb --ignore-existing minio-1/admin-bucket
              for bucket in $$(tr ',' '\n' <<< "iceberg-bucket,hive-bucket,landing-bucket")
        do
          mc mb --ignore-existing minio-1/$$bucket
        done
        #
        while [ 1 -eq 1 ];do sleep 60;done
    restart: unless-stopped
  #  ================================== Grafana ========================================== #
  grafana:
    image: grafana/grafana:12.4.0-20977568970-ubuntu
    hostname: grafana
    container_name: grafana
    labels:
      com.platys.name: grafana
      com.platys.description: Data visualization and dashboarding
      com.platys.webui.title: Grafana UI
      com.platys.webui.url: http://dataplatform:3000
      com.platys.restapi.title: Grafana API
      com.platys.restapi.url: http://dataplatform:3000/api/org
    expose:
      - 3000
    ports:
      - 3000:3000
    environment:
      - GF_SERVER_HTTP_ADDR=0.0.0.0
      - GF_SERVER_HTTP_PORT=3000
      - GF_AUTH_ANONYMOUS_ENABLED=False
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=abc123!
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/grafana/dashboards/dashboard.yml:/etc/grafana/provisioning/dashboards/dashboard.yml
      - ./scripts/grafana/dashboards/docker/monitor_services.json:/etc/grafana/provisioning/dashboards/docker/monitor_services.json
      - ./scripts/grafana/dashboards/kafka/confluent-platform-kraft.json:/etc/grafana/provisioning/dashboards/kafka/confluent-platform-kraft.json
      - ./scripts/grafana/dashboards/kafka/kafka-cluster-kraft.json:/etc/grafana/provisioning/dashboards/kafka/kafka-cluster-kraft.json
      - ./scripts/grafana/dashboards/kafka/kafka-topics-kraft.json:/etc/grafana/provisioning/dashboards/kafka/kafka-topics-kraft.json
      - ./scripts/grafana/dashboards/kafka/kraft.json:/etc/grafana/provisioning/dashboards/kafka/kraft.json
      - ./scripts/grafana/dashboards/kafka/cluster-linking.json:/etc/grafana/provisioning/dashboards/kafka/cluster-linking.json
      - ./scripts/grafana/dashboards/kafka-transaction-coordinator.json:/etc/grafana/provisioning/dashboards/kafka/kafka-transaction-coordinator.json
      - ./scripts/grafana/dashboards/kafka/kafka-connect-cluster.json:/etc/grafana/provisioning/dashboards/kafka/kafka-connect-cluster.json
      - ./scripts/grafana/dashboards/kafka/confluent-oracle-cdc.json:/etc/grafana/provisioning/dashboards/kafka/confluent-oracle-cdc.json
      - ./scripts/grafana/dashboards/kafka/ksqldb-cluster.json:/etc/grafana/provisioning/dashboards/kafka/ksqldb-cluster.json
      - ./scripts/grafana/dashboards/kafka/schema-registry-cluster.json:/etc/grafana/provisioning/dashboards/kafka/schema-registry-cluster.json
      - ./scripts/grafana/dashboards/kafka/kafka-topics.json:/etc/grafana/provisioning/dashboards/kafka/kafka-topics.json
      - ./scripts/grafana/dashboards/kafka/kafka-lag-exporter.json:/etc/grafana/provisioning/dashboards/kafka/kafka-lag-exporter.json
      - ./scripts/grafana/dashboards/kafka/kafka-consumer.json:/etc/grafana/provisioning/dashboards/kafka/kafka-consumer.json
      - ./scripts/grafana/dashboards/kafka/kafka-producer.json:/etc/grafana/provisioning/dashboards/kafka/kafka-producer.json
      - ./scripts/grafana/dashboards/kafka/kafka-quotas.json:/etc/grafana/provisioning/dashboards/kafka/kafka-quotas.json
      - ./scripts/grafana/dashboards/kafka/kafka-stream.json:/etc/grafana/provisioning/dashboards/kafka/kafka-stream.json
      - ./scripts/grafana/dashboards/kafka/kafka-streams-rocksdb.json:/etc/grafana/provisioning/dashboards/kafka/kafka-streams-rocksdb.json
      - ./scripts/grafana/dashboards/kafka/OTel_kafkametrics.json:/etc/grafana/provisioning/dashboards/kafka/OTel_kafkametrics.json
      - ./scripts/grafana/dashboards/elasticsearch:/etc/grafana/provisioning/dashboards/elasticsearch
      - ./scripts/grafana/dashboards/postgresql:/etc/grafana/provisioning/dashboards/postgresql
      - ./scripts/grafana/dashboards/minio:/etc/grafana/provisioning/dashboards/minio
      - ./scripts/grafana/dashboards/spark:/etc/grafana/provisioning/dashboards/spark
      - ./scripts/grafana/datasources/prometheus-datasource.yml:/etc/grafana/provisioning/datasources/prometheus-datasource.yml
    restart: unless-stopped
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/3000; exit $$?;'
      interval: 1s
      timeout: 5s
      retries: 5
  #  ================================== markdown-viewer ========================================== #
  markdown-viewer:
    image: dannyben/madness:latest
    container_name: markdown-viewer
    hostname: markdown-viewer
    labels:
      com.platys.name: markdown-viewer
      com.platys.description: Platys Platform homepage viewer
      com.platys.webui.title: Markdown Viewer UI
      com.platys.webui.url: http://dataplatform:80
    ports:
      - 80:3000
    volumes:
      - ./artefacts:/docs
      - ./conf/markdown-viewer/markdown-madness.yml:/docs/.madness.yml
      - ./data-transfer:/data-transfer
    command: server
    restart: unless-stopped
    healthcheck:
      test: [CMD-SHELL, curl -f http://markdown-viewer:3000 || exit 1]
      interval: 1m30s
      timeout: 10s
      retries: 3
      start_period: 1m
  markdown-renderer:
    image: trivadis/jinja2-renderer:0.8.1
    container_name: markdown-renderer
    hostname: markdown-renderer
    labels:
      com.platys.name: markdown-renderer
      com.platys.description: Platys Platform homepage rendering
    environment:
      USE_PUBLIC_IP: 'True'
      PUBLIC_IP: ${PUBLIC_IP}
      DOCKER_HOST_IP: ${DOCKER_HOST_IP}
      DATAPLATFORM_HOME: ${DATAPLATFORM_HOME}
      PLATYS_PLATFORM_NAME: platys-platform
      PLATYS_PLATFORM_STACK: trivadis/platys-modern-data-platform
      PLATYS_PLATFORM_STACK_VERSION: develop
      PLATYS_COPY_COOKBOOK_DATA: 'True'
      PLATYS_DATACENTER:
      SERVICE_LIST_VERSION: 2
    volumes:
      - ./artefacts/templates:/templates
      - ./artefacts/templates:/scripts
      - .:/variables
      - ./artefacts:/output
      - ./data-transfer:/data-transfer
    init: true
volumes:
  data-transfer-vol:
    name: data_transfer_vol
  aws-credentials-vol:
  spark-3-5-3-vol:
  spark-3-5-3-java-vol:
